{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3495ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91d7010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Small epsilon value for stabilizing division operations used in standardization\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1724b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the architecture, we will assume that the policy head will output the logits of the action probabilites\n",
    "\n",
    "def build_model(learning_rate = 0.01):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(64, activation = 'relu')(inputs)   \n",
    "    outputs_policy = layers.Dense(2, activation = 'linear')(x) \n",
    "    outputs_value = layers.Dense(1, activation = 'linear')(x)\n",
    "    network = keras.Model(inputs=inputs, outputs=[outputs_policy, outputs_value])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6957808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all operations to work we need to change some datatypes\n",
    "\n",
    "def env_step(action):\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action):\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ef8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that runs an episode outputting the results\n",
    "\n",
    "def run_episode(initial_state, model, max_steps):\n",
    "        \n",
    "    states = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    # Convert state into a batched tensor (batch size = 1) to feed it to the network\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    \n",
    "    states.write(0, state)\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        # Convert state into a batched tensor (batch size = 1) to feed it to the network\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        states = states.write(t + 1, state)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    states = states.stack()\n",
    "    \n",
    "    return action_probs, values, rewards, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee2044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_estimates(rewards, model, gamma, standardize = False):\n",
    "    n = tf.shape(rewards)[0]\n",
    "    q_estimates = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate estimate the Q-values\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        reward = rewards[i]\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_sum.set_shape(discounted_sum_shape)\n",
    "        q_estimates = q_estimates.write(i, discounted_sum)\n",
    "    q_estimates = q_estimates.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        q_estimates = ((q_estimates - tf.math.reduce_mean(q_estimates)) / (tf.math.reduce_std(q_estimates) + eps))\n",
    "\n",
    "    return q_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e141f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(action_probs, values, q_estimates):\n",
    "    advantage = q_estimates - values\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(advantage * q_estimates)\n",
    "\n",
    "    critic_loss = tf.math.reduce_sum((q_estimates - values)**2)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ec01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(initial_state, model, optimizer, gamma, max_steps_per_episode):\n",
    "\n",
    "    #with tf.GradientTape() as tape:\n",
    "                    \n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards, states = run_episode(initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate expected returns\n",
    "    q_values = get_q_estimates(rewards, model, gamma, standardize = False)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, q_values = [tf.expand_dims(x, 1) for x in [action_probs, values, q_values]] \n",
    "\n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, q_values)\n",
    "            \n",
    "    # Calculate episode reward\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        #grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the model's parameters\n",
    "    #optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return episode_reward, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a630b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 4151:  42%|█████████▌             | 4151/10000 [00:53<01:15, 77.85it/s, episode_reward=23, running_reward=17.8]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mset_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1426\u001b[0m                                  for key in postfix.keys())\n\u001b[0;32m   1427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1428\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1492\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mprint_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36mfp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m             \u001b[0mfp_flush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\utils.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m                     \u001b[1;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                     \u001b[1;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 2000\n",
    "\n",
    "#Build the model\n",
    "network = build_model()\n",
    "\n",
    "# Choose learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Cartpole is considered solved if average reward is >= 300 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 250\n",
    "running_reward = 0\n",
    "\n",
    "# Choose size of batches of gradients\n",
    "batch_update_size = 48\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        for i in t:\n",
    "            # Define a common loss \n",
    "            loss = tf.constant(0.)\n",
    "\n",
    "            # Start with defining the initial state\n",
    "            initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "\n",
    "            episode_reward, loss_step = train_step(initial_state, network, optimizer, gamma, max_steps_per_episode)\n",
    "            episode_reward = int(episode_reward)\n",
    "\n",
    "            # Update loss\n",
    "            loss = loss + loss_step/batch_update_size\n",
    "\n",
    "            episodes_reward.append(episode_reward)\n",
    "            running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "            # Update network every 'batch_update_size' episodes\n",
    "            if i % batch_update_size == 0:\n",
    "                    grads = tape.gradient(loss, network.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "                    loss = tf.constant(0.)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "            if i % 10 == 0:\n",
    "                pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "            if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "                break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d17d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
