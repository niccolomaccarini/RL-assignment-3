{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3495ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import numpy as np\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from typing import Any, List, Sequence, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e91d7010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikma\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Small epsilon value for stabilizing division operations used in standardization\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1724b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the architecture, we will assume that the policy head will output the logits of the action probabilites\n",
    "\n",
    "def build_model(learning_rate = 0.01):\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    x = layers.Dense(64, activation = 'relu')(inputs)   \n",
    "    outputs_policy = layers.Dense(2, activation = 'linear')(x) \n",
    "    outputs_value = layers.Dense(1, activation = 'linear')(x)\n",
    "    network = keras.Model(inputs=inputs, outputs=[outputs_policy, outputs_value])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6957808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all operations to work we need to change some datatypes\n",
    "\n",
    "def env_step(action):\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    return (state.astype(np.float32), np.array(reward, np.int32), np.array(done, np.int32))\n",
    "\n",
    "\n",
    "def tf_env_step(action):\n",
    "    return tf.numpy_function(env_step, [action], [tf.float32, tf.int32, tf.int32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ef8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function that runs an episode outputting the results\n",
    "\n",
    "def run_episode(initial_state, model, max_steps):\n",
    "        \n",
    "    states = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "\n",
    "    initial_state_shape = initial_state.shape\n",
    "    state = initial_state\n",
    "    # Convert state into a batched tensor (batch size = 1) to feed it to the network\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    \n",
    "    states.write(0, state)\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0, action])\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done = tf_env_step(action)\n",
    "        state.set_shape(initial_state_shape)\n",
    "        # Convert state into a batched tensor (batch size = 1) to feed it to the network\n",
    "        state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        states = states.write(t + 1, state)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if tf.cast(done, tf.bool):\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "    states = states.stack()\n",
    "    \n",
    "    return action_probs, values, rewards, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee2044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_estimates(rewards, model, gamma, N, states, standardize = False):\n",
    "    n = tf.shape(rewards)[0]\n",
    "    q_estimates = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "    # Start from the end of `rewards` and accumulate estimate the Q-values\n",
    "    rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "    discounted_sum = tf.constant(0.0)\n",
    "    discounted_sum_shape = discounted_sum.shape\n",
    "    for i in tf.range(n):\n",
    "        T = tf.math.minimum(N, i)\n",
    "        reward = rewards[i]\n",
    "        bootstrapped_value = model(states[n-i-1 + T])[1]\n",
    "        if i < N:\n",
    "            discounted_sum = reward + gamma * discounted_sum\n",
    "            discounted_sum.set_shape(discounted_sum_shape)\n",
    "        q_estimates = q_estimates.write(i, discounted_sum + bootstrapped_value)\n",
    "        \n",
    "    q_estimates = q_estimates.stack()[::-1]\n",
    "\n",
    "    if standardize:\n",
    "        q_estimates = ((q_estimates - tf.math.reduce_mean(q_estimates)) / (tf.math.reduce_std(q_estimates) + eps))\n",
    "\n",
    "    return q_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e141f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(action_probs, values, q_estimates):\n",
    "    advantage = q_estimates - values\n",
    "    action_log_probs = tf.math.log(action_probs)\n",
    "    actor_loss = -tf.math.reduce_sum(advantage * q_estimates)\n",
    "\n",
    "    critic_loss = tf.math.reduce_sum((q_estimates - values)**2)\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ec01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(initial_state, model, optimizer, gamma, max_steps_per_episode, N):\n",
    "\n",
    "    #with tf.GradientTape() as tape:\n",
    "                    \n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards, states = run_episode(initial_state, model, max_steps_per_episode) \n",
    "\n",
    "    # Calculate expected returns\n",
    "    q_values = get_q_estimates(rewards, model, gamma, N, states, standardize = False)\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    action_probs, values, q_values = [tf.expand_dims(x, 1) for x in [action_probs, values, q_values]] \n",
    "\n",
    "    # Calculating loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, q_values)\n",
    "            \n",
    "    # Calculate episode reward\n",
    "    episode_reward = tf.math.reduce_sum(rewards)\n",
    "\n",
    "        # Compute the gradients from the loss\n",
    "        #grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the model's parameters\n",
    "    #optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return episode_reward, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a630b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_episodes_criterion = 100\n",
    "max_episodes = 10000\n",
    "max_steps_per_episode = 2000\n",
    "\n",
    "#Build the model\n",
    "network = build_model()\n",
    "\n",
    "# Choose learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Cartpole is considered solved if average reward is >= 300 over 100 \n",
    "# consecutive trials\n",
    "reward_threshold = 250\n",
    "running_reward = 0\n",
    "\n",
    "# Bootstrap after steps\n",
    "N = 48\n",
    "\n",
    "# Choose size of batches of gradients\n",
    "batch_update_size = 24\n",
    "\n",
    "# Choose optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep last episodes reward\n",
    "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
    "\n",
    "with tqdm.trange(max_episodes) as t:\n",
    "    with tf.GradientTape(persistent = True) as tape:\n",
    "        for i in t:\n",
    "            # Define a common loss \n",
    "            loss = tf.constant(0.)\n",
    "\n",
    "            # Start with defining the initial state\n",
    "            initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "\n",
    "            episode_reward, loss_step = train_step(initial_state, network, optimizer, gamma, max_steps_per_episode, N)\n",
    "            episode_reward = int(episode_reward)\n",
    "\n",
    "            # Update loss\n",
    "            loss = loss + loss_step/batch_update_size\n",
    "\n",
    "            episodes_reward.append(episode_reward)\n",
    "            running_reward = statistics.mean(episodes_reward)\n",
    "\n",
    "            t.set_description(f'Episode {i}')\n",
    "            t.set_postfix(episode_reward=episode_reward, running_reward=running_reward)\n",
    "\n",
    "            # Update network every 'batch_update_size' episodes\n",
    "            if i % batch_update_size == 0:\n",
    "                    grads = tape.gradient(loss, network.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "                    loss = tf.constant(0.)\n",
    "\n",
    "            # Show average episode reward every 10 episodes\n",
    "            if i % 10 == 0:\n",
    "                pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "            if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
    "                break\n",
    "\n",
    "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d17d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
